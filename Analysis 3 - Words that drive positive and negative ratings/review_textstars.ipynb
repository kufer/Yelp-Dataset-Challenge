{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import random\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk import *\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Set up a connection with the server\n",
    "conn = pymysql.connect(host='localhost', port=8889, user='root', passwd='root')\n",
    "# Set up a cursor object that will serve as a virtual 'cursor'\n",
    "cursor = conn.cursor()\n",
    "conn.commit()\n",
    "\n",
    "# Fetch text and stars data (list of tuples)\n",
    "cursor.execute('USE YELP_DATA_PROJECT;')\n",
    "cursor.execute('SELECT text, stars FROM usa_reviews WHERE length(text) > 50;') # Querying only if length of review is greater than 50 chars, otherwise review is hard to accommodate in a bag of words.\n",
    "textstars=list(cursor.fetchall())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data\n",
      "Data size = 5000\n",
      "Average review length (characters) = 678.4928\n",
      "Average # of stars = 3.7216\n",
      "Star values (1 to 5) = [449, 489, 751, 1627, 1684]\n",
      "\n",
      "Test data\n",
      "Data size = 5000\n",
      "Average review length (characters) = 668.4844\n",
      "Average # of stars = 3.6792\n",
      "Star values (1 to 5) = [479, 513, 776, 1597, 1635]\n"
     ]
    }
   ],
   "source": [
    "# Get a sample of the entire set of reviews, to avoid excessive computational cost.\n",
    "randsamp = np.asarray(random.sample(range(1,len(textstars)+1),10000))\n",
    "trainsamp = np.split(randsamp,2)[0]\n",
    "testsamp = np.split(randsamp,2)[1]\n",
    "\n",
    "# Get train data\n",
    "textstars_sample_train = np.asarray(textstars)[trainsamp]\n",
    "text_train = [i[0] for i in textstars_sample_train]\n",
    "stars_train = [float(i[1]) for i in textstars_sample_train]\n",
    "\n",
    "# A few statistics about the data being used\n",
    "print('Train data')\n",
    "print('Data size =', len(textstars_sample_train))\n",
    "print('Average review length (characters) =',sum([len(r) for r in text_train])/len(text_train))\n",
    "print('Average # of stars =',sum(stars_train)/len(stars_train))\n",
    "print('Star values (1 to 5) =',[stars_train.count(i) for i in range(1,6)])\n",
    "\n",
    "# Read test data\n",
    "textstars_sample_test = np.asarray(textstars)[testsamp]\n",
    "text_test = [i[0] for i in textstars_sample_test]\n",
    "stars_test = [float(i[1]) for i in textstars_sample_test]\n",
    "\n",
    "# A few statistics about the test data being used\n",
    "print('\\nTest data')\n",
    "print('Data size =', len(textstars_sample_test))\n",
    "print('Average review length (characters) =',sum([len(r) for r in text_test])/len(text_test))\n",
    "print('Average # of stars =',sum(stars_test)/len(stars_test))\n",
    "print('Star values (1 to 5) =',[stars_test.count(i) for i in range(1,6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stem\n",
    "def stem(word):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    temp_review=wnl.lemmatize(word,'v')\n",
    "    return wnl.lemmatize(temp_review,'n')\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(review):\n",
    "    tok_review = TreebankWordTokenizer().tokenize(review)\n",
    "    return tok_review\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stop(review):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    result_text= [w for w in review if not w in stopset]\n",
    "    return result_text\n",
    "\n",
    "# Define a comprehensive preprocessing function\n",
    "def preprocess(rev_text): # list of strings\n",
    "    tokenized_reviews = [[] for _ in range(len(rev_text))]\n",
    "    stemmed_reviews = [[] for _ in range(len(rev_text))]\n",
    "    tok_stem_reviews = [[] for _ in range(len(rev_text))]\n",
    "    tok_stem_stop_reviews = [[] for _ in range(len(rev_text))]\n",
    "    preproc_reviews = [[] for _ in range(len(rev_text))]\n",
    "\n",
    "    rev_text = [i.lower() for i in rev_text] # Lower case\n",
    "    for i in range(len(rev_text)):\n",
    "        tokenized_reviews[i] = tokenize(rev_text[i]) # Tokenize the reviews -- Takes as input a string\n",
    "        for j in range(len(tokenized_reviews[i])): \n",
    "            stemmed_reviews[i].append(stem(tokenized_reviews[i][j])) # Stem reviews\n",
    "        tok_stem_stop_reviews[i]=remove_stop(stemmed_reviews[i]) # Remove stop words\n",
    "        for j in range(len(tok_stem_stop_reviews[i])): \n",
    "            if(tok_stem_stop_reviews[i][j].isalpha()): # Only keep words with only letters\n",
    "                preproc_reviews[i].append(tok_stem_stop_reviews[i][j])\n",
    "    for i in range(len(preproc_reviews)):\n",
    "        preproc_reviews[i] = \" \".join(preproc_reviews[i]) # Ready to generate bag of words\n",
    "    \n",
    "    return preproc_reviews # list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess train set of reviews\n",
    "preprocessed_text_train = preprocess(text_train)\n",
    "\n",
    "# Generate bag of words (train)\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             max_features = 500) \n",
    "bow_train = vectorizer.fit_transform(preprocessed_text_train) # Learn the vocabulary, create feature vectors\n",
    "bow_train = bow_train.toarray() # Numpy arrays are easy to work with, so convert the result to an array\n",
    "\n",
    "# Create and sort a dict vocabulary, with all expressions included in the bag of words and their counts\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocabulary = dict()\n",
    "dist = np.sum(bow_train, axis=0)\n",
    "for expression, count in zip(vocab, dist):\n",
    "    vocabulary.update({expression: count})\n",
    "vocabulary = sorted(vocabulary.items(), key=lambda x:x[1])\n",
    "\n",
    "stars_train = np.asarray(stars_train).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess test set of reviews\n",
    "preprocessed_text_test = preprocess(text_test)\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "bow_test = vectorizer.transform(preprocessed_text_test)\n",
    "bow_test = bow_test.toarray()\n",
    "\n",
    "stars_test = np.asarray(stars_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create linear regression object, train the model\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(bow_train, stars_train.ravel())\n",
    "\n",
    "# Create a random forest regressor with 100 trees, train the model\n",
    "#forest = RandomForestRegressor(n_estimators = 200) \n",
    "#forest = forest.fit(bow_train, stars_train.ravel())\n",
    "\n",
    "# Create a SVR object, train the model\n",
    "#supvec = svm.SVR(kernel = 'linear')\n",
    "#supvec.fit(bow_train, stars_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate predictions based on the bag of words (test).\n",
    "stars_pred_linreg = np.asarray(linreg.predict(bow_test))\n",
    "#stars_pred_rf = np.asarray(forest.predict(bow_test))\n",
    "#stars_pred_supvec = np.asarray(supvec.predict(bow_test))\n",
    "\n",
    "# Bound predictions to [1,5]\n",
    "for i in range(len(stars_pred_linreg)): \n",
    "    if stars_pred_linreg[i]<1:\n",
    "        stars_pred_linreg[i] = 1\n",
    "    elif stars_pred_linreg[i]>5:\n",
    "        stars_pred_linreg[i] = 5\n",
    "        \n",
    "#for i in range(len(stars_pred_rf)): \n",
    "#    if stars_pred_rf[i]<1:\n",
    "#        stars_pred_rf[i] = 1\n",
    "#    elif stars_pred_rf[i]>5:\n",
    "#        stars_pred_rf[i] = 5\n",
    "\n",
    "#for i in range(len(stars_pred_supvec)): \n",
    "#    if stars_pred_supvec[i]<1:\n",
    "#        stars_pred_supvec[i] = 1\n",
    "#    elif stars_pred_supvec[i]>5:\n",
    "#        stars_pred_supvec[i] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinReg\n",
      "RMSE = 1.4344\n",
      "MAE = 1.1571\n",
      "AdjR2 = 0.2683\n"
     ]
    }
   ],
   "source": [
    "# Compute RMSE and MAE for each of the models.\n",
    "def rmse(pred, actual):\n",
    "    return np.sqrt(np.mean((pred-actual)**2))\n",
    "\n",
    "rmse_linreg = rmse(stars_pred_linreg, stars_test)\n",
    "mae_linreg = np.mean(abs(stars_pred_linreg - stars_test))\n",
    "r2_linreg = r2_score(stars_test, stars_pred_linreg)\n",
    "adjr2_linreg = 1-((len(bow_test)-1)/(len(bow_test)-len(vocab)-1))*(1-r2_linreg)\n",
    "\n",
    "print('LinReg')\n",
    "print('RMSE = %.4f' % rmse_linreg)\n",
    "print('MAE = %.4f' % mae_linreg)\n",
    "print('AdjR2 = %.4f' % adjr2_linreg)\n",
    "\n",
    "#rmse_rf = rmse(stars_pred_rf, stars_test)\n",
    "#mae_rf = np.mean(abs(stars_pred_rf - stars_test))\n",
    "#print('\\nRF')\n",
    "#print('RMSE = %.4f' % rmse_rf)\n",
    "#print('MAE = %.4f' % mae_rf)\n",
    "\n",
    "#rmse_supvec = rmse(stars_pred_supvec, stars_test)\n",
    "#mae_supvec = np.mean(abs(stars_pred_supvec - stars_test))\n",
    "#print('\\nPoly-2 SVM')\n",
    "#print('RMSE = %.4f' % rmse_supvec)\n",
    "#print('MAE = %.4f' % mae_supvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importance = dict()\n",
    "for i in range(len(vocab)):\n",
    "    importance.update({vocab[i]: linreg.coef_[i]})\n",
    "importance = sorted(importance.items(), key=lambda x:x[1])\n",
    "\n",
    "mostpos = importance[-10:]\n",
    "mostneg = importance[:10]\n",
    "\n",
    "words_pos = [i[0] for i in mostpos]\n",
    "coeff_pos = [i[1] for i in mostpos]\n",
    "y_pos = np.arange(len(mostpos))\n",
    "\n",
    "plt.barh(y_pos, coeff_pos, align='center', color='green')\n",
    "plt.yticks(y_pos, words_pos)\n",
    "plt.xlabel('Coefficient')\n",
    "plt.title('Which words drive positive ratings the most?')\n",
    "plt.show()\n",
    "\n",
    "words_neg = [i[0] for i in mostneg]\n",
    "coeff_neg = [-i[1] for i in mostneg]\n",
    "y_neg = np.arange(len(mostneg))\n",
    "\n",
    "plt.barh(y_neg, coeff_neg, align='center', color='red')\n",
    "plt.yticks(y_neg, words_neg)\n",
    "plt.xlabel('Coefficient (-1)')\n",
    "plt.title('Which words drive negative ratings the most?')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
